{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2613096,"sourceType":"datasetVersion","datasetId":1379274}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-27T19:54:46.224485Z","iopub.execute_input":"2024-03-27T19:54:46.224872Z","iopub.status.idle":"2024-03-27T19:54:47.455890Z","shell.execute_reply.started":"2024-03-27T19:54:46.224835Z","shell.execute_reply":"2024-03-27T19:54:47.454726Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/bank-marketing-campaign/bank.csv\n/kaggle/input/bank-marketing-campaign/bank-full.csv\n/kaggle/input/bank-marketing-campaign/bank-full - Copy.csv\n/kaggle/input/bank-marketing-campaign/bank-names - Copy.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV, cross_val_score","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:47.458156Z","iopub.execute_input":"2024-03-27T19:54:47.458755Z","iopub.status.idle":"2024-03-27T19:54:49.475247Z","shell.execute_reply.started":"2024-03-27T19:54:47.458717Z","shell.execute_reply":"2024-03-27T19:54:49.474332Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Data Description:\n- 1 - age (numeric)\n- 2 - job: type of job (categorical): \n- 3 - marital: marital status (categorical):\n- 4 - education (categorical):\n- 5 - default: has credit in default? (binary):\n- 6 - balance: average yearly balance, in euros (numeric)\n- 7 - housing: has housing loan? (binary):\n- 8 - loan: has personal loan? (binary):\n- 9 - contact: contact communication type (categorical):\n- 10 - day: last contact day of the month (numeric)\n- 11 - month: last contact month of year (categorical):\n- 12 - duration: last contact duration, in seconds (numeric)\n- 13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n- 14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\n- 15 - previous: number of contacts performed before this campaign and for this client (numeric)\n- 16 - poutcome: outcome of the previous marketing campaign (categorical):\n- 17 - y: has the client subscribed a term deposit? (binary):\n  - \"yes\"\n  - \"no\"\n","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/bank-marketing-campaign/bank-full.csv', sep=';')\ntest = pd.read_csv('../input/bank-marketing-campaign/bank.csv', sep=';')","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:49.476578Z","iopub.execute_input":"2024-03-27T19:54:49.477668Z","iopub.status.idle":"2024-03-27T19:54:49.662609Z","shell.execute_reply.started":"2024-03-27T19:54:49.477635Z","shell.execute_reply":"2024-03-27T19:54:49.661301Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:49.664812Z","iopub.execute_input":"2024-03-27T19:54:49.665172Z","iopub.status.idle":"2024-03-27T19:54:49.689606Z","shell.execute_reply.started":"2024-03-27T19:54:49.665142Z","shell.execute_reply":"2024-03-27T19:54:49.688792Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   age           job  marital  education default  balance housing loan  \\\n0   58    management  married   tertiary      no     2143     yes   no   \n1   44    technician   single  secondary      no       29     yes   no   \n2   33  entrepreneur  married  secondary      no        2     yes  yes   \n3   47   blue-collar  married    unknown      no     1506     yes   no   \n4   33       unknown   single    unknown      no        1      no   no   \n\n   contact  day month  duration  campaign  pdays  previous poutcome   y  \n0  unknown    5   may       261         1     -1         0  unknown  no  \n1  unknown    5   may       151         1     -1         0  unknown  no  \n2  unknown    5   may        76         1     -1         0  unknown  no  \n3  unknown    5   may        92         1     -1         0  unknown  no  \n4  unknown    5   may       198         1     -1         0  unknown  no  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>job</th>\n      <th>marital</th>\n      <th>education</th>\n      <th>default</th>\n      <th>balance</th>\n      <th>housing</th>\n      <th>loan</th>\n      <th>contact</th>\n      <th>day</th>\n      <th>month</th>\n      <th>duration</th>\n      <th>campaign</th>\n      <th>pdays</th>\n      <th>previous</th>\n      <th>poutcome</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>58</td>\n      <td>management</td>\n      <td>married</td>\n      <td>tertiary</td>\n      <td>no</td>\n      <td>2143</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>unknown</td>\n      <td>5</td>\n      <td>may</td>\n      <td>261</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>44</td>\n      <td>technician</td>\n      <td>single</td>\n      <td>secondary</td>\n      <td>no</td>\n      <td>29</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>unknown</td>\n      <td>5</td>\n      <td>may</td>\n      <td>151</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>33</td>\n      <td>entrepreneur</td>\n      <td>married</td>\n      <td>secondary</td>\n      <td>no</td>\n      <td>2</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>unknown</td>\n      <td>5</td>\n      <td>may</td>\n      <td>76</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>47</td>\n      <td>blue-collar</td>\n      <td>married</td>\n      <td>unknown</td>\n      <td>no</td>\n      <td>1506</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>unknown</td>\n      <td>5</td>\n      <td>may</td>\n      <td>92</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33</td>\n      <td>unknown</td>\n      <td>single</td>\n      <td>unknown</td>\n      <td>no</td>\n      <td>1</td>\n      <td>no</td>\n      <td>no</td>\n      <td>unknown</td>\n      <td>5</td>\n      <td>may</td>\n      <td>198</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>no</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:49.690775Z","iopub.execute_input":"2024-03-27T19:54:49.691258Z","iopub.status.idle":"2024-03-27T19:54:49.740584Z","shell.execute_reply.started":"2024-03-27T19:54:49.691232Z","shell.execute_reply":"2024-03-27T19:54:49.739429Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 45211 entries, 0 to 45210\nData columns (total 17 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   age        45211 non-null  int64 \n 1   job        45211 non-null  object\n 2   marital    45211 non-null  object\n 3   education  45211 non-null  object\n 4   default    45211 non-null  object\n 5   balance    45211 non-null  int64 \n 6   housing    45211 non-null  object\n 7   loan       45211 non-null  object\n 8   contact    45211 non-null  object\n 9   day        45211 non-null  int64 \n 10  month      45211 non-null  object\n 11  duration   45211 non-null  int64 \n 12  campaign   45211 non-null  int64 \n 13  pdays      45211 non-null  int64 \n 14  previous   45211 non-null  int64 \n 15  poutcome   45211 non-null  object\n 16  y          45211 non-null  object\ndtypes: int64(7), object(10)\nmemory usage: 5.9+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:49.742225Z","iopub.execute_input":"2024-03-27T19:54:49.742914Z","iopub.status.idle":"2024-03-27T19:54:49.750520Z","shell.execute_reply.started":"2024-03-27T19:54:49.742873Z","shell.execute_reply":"2024-03-27T19:54:49.749431Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(45211, 17)"},"metadata":{}}]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:49.751873Z","iopub.execute_input":"2024-03-27T19:54:49.752541Z","iopub.status.idle":"2024-03-27T19:54:49.811054Z","shell.execute_reply.started":"2024-03-27T19:54:49.752502Z","shell.execute_reply":"2024-03-27T19:54:49.809960Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                age        balance           day      duration      campaign  \\\ncount  45211.000000   45211.000000  45211.000000  45211.000000  45211.000000   \nmean      40.936210    1362.272058     15.806419    258.163080      2.763841   \nstd       10.618762    3044.765829      8.322476    257.527812      3.098021   \nmin       18.000000   -8019.000000      1.000000      0.000000      1.000000   \n25%       33.000000      72.000000      8.000000    103.000000      1.000000   \n50%       39.000000     448.000000     16.000000    180.000000      2.000000   \n75%       48.000000    1428.000000     21.000000    319.000000      3.000000   \nmax       95.000000  102127.000000     31.000000   4918.000000     63.000000   \n\n              pdays      previous  \ncount  45211.000000  45211.000000  \nmean      40.197828      0.580323  \nstd      100.128746      2.303441  \nmin       -1.000000      0.000000  \n25%       -1.000000      0.000000  \n50%       -1.000000      0.000000  \n75%       -1.000000      0.000000  \nmax      871.000000    275.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>balance</th>\n      <th>day</th>\n      <th>duration</th>\n      <th>campaign</th>\n      <th>pdays</th>\n      <th>previous</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>45211.000000</td>\n      <td>45211.000000</td>\n      <td>45211.000000</td>\n      <td>45211.000000</td>\n      <td>45211.000000</td>\n      <td>45211.000000</td>\n      <td>45211.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>40.936210</td>\n      <td>1362.272058</td>\n      <td>15.806419</td>\n      <td>258.163080</td>\n      <td>2.763841</td>\n      <td>40.197828</td>\n      <td>0.580323</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>10.618762</td>\n      <td>3044.765829</td>\n      <td>8.322476</td>\n      <td>257.527812</td>\n      <td>3.098021</td>\n      <td>100.128746</td>\n      <td>2.303441</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>18.000000</td>\n      <td>-8019.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>33.000000</td>\n      <td>72.000000</td>\n      <td>8.000000</td>\n      <td>103.000000</td>\n      <td>1.000000</td>\n      <td>-1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>39.000000</td>\n      <td>448.000000</td>\n      <td>16.000000</td>\n      <td>180.000000</td>\n      <td>2.000000</td>\n      <td>-1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>48.000000</td>\n      <td>1428.000000</td>\n      <td>21.000000</td>\n      <td>319.000000</td>\n      <td>3.000000</td>\n      <td>-1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>95.000000</td>\n      <td>102127.000000</td>\n      <td>31.000000</td>\n      <td>4918.000000</td>\n      <td>63.000000</td>\n      <td>871.000000</td>\n      <td>275.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:49.812921Z","iopub.execute_input":"2024-03-27T19:54:49.813270Z","iopub.status.idle":"2024-03-27T19:54:49.822749Z","shell.execute_reply.started":"2024-03-27T19:54:49.813242Z","shell.execute_reply":"2024-03-27T19:54:49.821244Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"age           int64\njob          object\nmarital      object\neducation    object\ndefault      object\nbalance       int64\nhousing      object\nloan         object\ncontact      object\nday           int64\nmonth        object\nduration      int64\ncampaign      int64\npdays         int64\nprevious      int64\npoutcome     object\ny            object\ndtype: object"},"metadata":{}}]},{"cell_type":"code","source":"# Change data types\ntrain['job'] = train['job'].astype('category')\ntrain['marital'] = train['marital'].astype('category')\ntrain['education'] = train['education'].astype('category')\ntrain['contact'] = train['contact'].astype('category')\ntrain['month'] = train['month'].astype('category')\ntrain['poutcome'] = train['poutcome'].astype('category')\ntrain['y'] = train['y'].astype('category')","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:49.824901Z","iopub.execute_input":"2024-03-27T19:54:49.825429Z","iopub.status.idle":"2024-03-27T19:54:49.862868Z","shell.execute_reply.started":"2024-03-27T19:54:49.825388Z","shell.execute_reply":"2024-03-27T19:54:49.862013Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Change data types\ntest['job'] = test['job'].astype('category')\ntest['marital'] = test['marital'].astype('category')\ntest['education'] = test['education'].astype('category')\ntest['contact'] = test['contact'].astype('category')\ntest['month'] = test['month'].astype('category')\ntest['poutcome'] = test['poutcome'].astype('category')\ntest['y'] = test['y'].astype('category')","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:49.867937Z","iopub.execute_input":"2024-03-27T19:54:49.868732Z","iopub.status.idle":"2024-03-27T19:54:49.884406Z","shell.execute_reply.started":"2024-03-27T19:54:49.868693Z","shell.execute_reply":"2024-03-27T19:54:49.883253Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Further exploration using histogram to check the distributions\ntrain.hist(bins = 20, figsize= (20,20))","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:49.885567Z","iopub.execute_input":"2024-03-27T19:54:49.885877Z","iopub.status.idle":"2024-03-27T19:54:51.979850Z","shell.execute_reply.started":"2024-03-27T19:54:49.885854Z","shell.execute_reply":"2024-03-27T19:54:51.978626Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array([[<Axes: title={'center': 'age'}>,\n        <Axes: title={'center': 'balance'}>,\n        <Axes: title={'center': 'day'}>],\n       [<Axes: title={'center': 'duration'}>,\n        <Axes: title={'center': 'campaign'}>,\n        <Axes: title={'center': 'pdays'}>],\n       [<Axes: title={'center': 'previous'}>, <Axes: >, <Axes: >]],\n      dtype=object)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 2000x2000 with 9 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"#Target Variable distribution\ntrain['y'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:51.981089Z","iopub.execute_input":"2024-03-27T19:54:51.981496Z","iopub.status.idle":"2024-03-27T19:54:51.993434Z","shell.execute_reply.started":"2024-03-27T19:54:51.981464Z","shell.execute_reply":"2024-03-27T19:54:51.992281Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"y\nno     39922\nyes     5289\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"This suggests that the dataset is imbalanced and we need to use resampling techniques or more robust algorithms like Random Forest, GBM to handle this efficiently.","metadata":{}},{"cell_type":"code","source":"train['job'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:51.994903Z","iopub.execute_input":"2024-03-27T19:54:51.995558Z","iopub.status.idle":"2024-03-27T19:54:52.010224Z","shell.execute_reply.started":"2024-03-27T19:54:51.995521Z","shell.execute_reply":"2024-03-27T19:54:52.008898Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"job\nblue-collar      9732\nmanagement       9458\ntechnician       7597\nadmin.           5171\nservices         4154\nretired          2264\nself-employed    1579\nentrepreneur     1487\nunemployed       1303\nhousemaid        1240\nstudent           938\nunknown           288\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train['marital'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.011900Z","iopub.execute_input":"2024-03-27T19:54:52.012434Z","iopub.status.idle":"2024-03-27T19:54:52.024140Z","shell.execute_reply.started":"2024-03-27T19:54:52.012399Z","shell.execute_reply":"2024-03-27T19:54:52.023031Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"marital\nmarried     27214\nsingle      12790\ndivorced     5207\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train['education'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.025813Z","iopub.execute_input":"2024-03-27T19:54:52.026410Z","iopub.status.idle":"2024-03-27T19:54:52.037968Z","shell.execute_reply.started":"2024-03-27T19:54:52.026371Z","shell.execute_reply":"2024-03-27T19:54:52.036815Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"education\nsecondary    23202\ntertiary     13301\nprimary       6851\nunknown       1857\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train['default'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.039617Z","iopub.execute_input":"2024-03-27T19:54:52.040628Z","iopub.status.idle":"2024-03-27T19:54:52.056450Z","shell.execute_reply.started":"2024-03-27T19:54:52.040590Z","shell.execute_reply":"2024-03-27T19:54:52.055377Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"default\nno     44396\nyes      815\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train['housing'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.058017Z","iopub.execute_input":"2024-03-27T19:54:52.058673Z","iopub.status.idle":"2024-03-27T19:54:52.074943Z","shell.execute_reply.started":"2024-03-27T19:54:52.058636Z","shell.execute_reply":"2024-03-27T19:54:52.073876Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"housing\nyes    25130\nno     20081\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train['loan'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.076561Z","iopub.execute_input":"2024-03-27T19:54:52.077230Z","iopub.status.idle":"2024-03-27T19:54:52.093748Z","shell.execute_reply.started":"2024-03-27T19:54:52.077193Z","shell.execute_reply":"2024-03-27T19:54:52.092748Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"loan\nno     37967\nyes     7244\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train['contact'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.095301Z","iopub.execute_input":"2024-03-27T19:54:52.095951Z","iopub.status.idle":"2024-03-27T19:54:52.107725Z","shell.execute_reply.started":"2024-03-27T19:54:52.095908Z","shell.execute_reply":"2024-03-27T19:54:52.106405Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"contact\ncellular     29285\nunknown      13020\ntelephone     2906\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train['poutcome'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.109190Z","iopub.execute_input":"2024-03-27T19:54:52.109820Z","iopub.status.idle":"2024-03-27T19:54:52.121382Z","shell.execute_reply.started":"2024-03-27T19:54:52.109786Z","shell.execute_reply":"2024-03-27T19:54:52.119876Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"poutcome\nunknown    36959\nfailure     4901\nother       1840\nsuccess     1511\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train['month'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.122987Z","iopub.execute_input":"2024-03-27T19:54:52.123792Z","iopub.status.idle":"2024-03-27T19:54:52.135433Z","shell.execute_reply.started":"2024-03-27T19:54:52.123751Z","shell.execute_reply":"2024-03-27T19:54:52.134284Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"month\nmay    13766\njul     6895\naug     6247\njun     5341\nnov     3970\napr     2932\nfeb     2649\njan     1403\noct      738\nsep      579\nmar      477\ndec      214\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train['age'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.137005Z","iopub.execute_input":"2024-03-27T19:54:52.137886Z","iopub.status.idle":"2024-03-27T19:54:52.151187Z","shell.execute_reply.started":"2024-03-27T19:54:52.137847Z","shell.execute_reply":"2024-03-27T19:54:52.149962Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"age\n32    2085\n31    1996\n33    1972\n34    1930\n35    1894\n      ... \n93       2\n90       2\n95       2\n88       2\n94       1\nName: count, Length: 77, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.153040Z","iopub.execute_input":"2024-03-27T19:54:52.153845Z","iopub.status.idle":"2024-03-27T19:54:52.176982Z","shell.execute_reply.started":"2024-03-27T19:54:52.153804Z","shell.execute_reply":"2024-03-27T19:54:52.175825Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"   age          job  marital  education default  balance housing loan  \\\n0   30   unemployed  married    primary      no     1787      no   no   \n1   33     services  married  secondary      no     4789     yes  yes   \n2   35   management   single   tertiary      no     1350     yes   no   \n3   30   management  married   tertiary      no     1476     yes  yes   \n4   59  blue-collar  married  secondary      no        0     yes   no   \n\n    contact  day month  duration  campaign  pdays  previous poutcome   y  \n0  cellular   19   oct        79         1     -1         0  unknown  no  \n1  cellular   11   may       220         1    339         4  failure  no  \n2  cellular   16   apr       185         1    330         1  failure  no  \n3   unknown    3   jun       199         4     -1         0  unknown  no  \n4   unknown    5   may       226         1     -1         0  unknown  no  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>job</th>\n      <th>marital</th>\n      <th>education</th>\n      <th>default</th>\n      <th>balance</th>\n      <th>housing</th>\n      <th>loan</th>\n      <th>contact</th>\n      <th>day</th>\n      <th>month</th>\n      <th>duration</th>\n      <th>campaign</th>\n      <th>pdays</th>\n      <th>previous</th>\n      <th>poutcome</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30</td>\n      <td>unemployed</td>\n      <td>married</td>\n      <td>primary</td>\n      <td>no</td>\n      <td>1787</td>\n      <td>no</td>\n      <td>no</td>\n      <td>cellular</td>\n      <td>19</td>\n      <td>oct</td>\n      <td>79</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>33</td>\n      <td>services</td>\n      <td>married</td>\n      <td>secondary</td>\n      <td>no</td>\n      <td>4789</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>cellular</td>\n      <td>11</td>\n      <td>may</td>\n      <td>220</td>\n      <td>1</td>\n      <td>339</td>\n      <td>4</td>\n      <td>failure</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>35</td>\n      <td>management</td>\n      <td>single</td>\n      <td>tertiary</td>\n      <td>no</td>\n      <td>1350</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>cellular</td>\n      <td>16</td>\n      <td>apr</td>\n      <td>185</td>\n      <td>1</td>\n      <td>330</td>\n      <td>1</td>\n      <td>failure</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>30</td>\n      <td>management</td>\n      <td>married</td>\n      <td>tertiary</td>\n      <td>no</td>\n      <td>1476</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>unknown</td>\n      <td>3</td>\n      <td>jun</td>\n      <td>199</td>\n      <td>4</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59</td>\n      <td>blue-collar</td>\n      <td>married</td>\n      <td>secondary</td>\n      <td>no</td>\n      <td>0</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>unknown</td>\n      <td>5</td>\n      <td>may</td>\n      <td>226</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>no</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test['y'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.178911Z","iopub.execute_input":"2024-03-27T19:54:52.179734Z","iopub.status.idle":"2024-03-27T19:54:52.192637Z","shell.execute_reply.started":"2024-03-27T19:54:52.179696Z","shell.execute_reply":"2024-03-27T19:54:52.191419Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"y\nno     4000\nyes     521\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Select numerical columns for correlation analysis\nnumerical_cols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n\n# Compute the correlation matrix\ncorrelation_matrix = train[numerical_cols].corr()\n\n# Create a heatmap to visualize the correlation matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\nplt.title('Correlation Matrix of Numerical Variables')\nplt.xlabel('Numerical Variables')\nplt.ylabel('Numerical Variables')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.194685Z","iopub.execute_input":"2024-03-27T19:54:52.195460Z","iopub.status.idle":"2024-03-27T19:54:52.623562Z","shell.execute_reply.started":"2024-03-27T19:54:52.195419Z","shell.execute_reply":"2024-03-27T19:54:52.622437Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x800 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"train.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.625176Z","iopub.execute_input":"2024-03-27T19:54:52.625828Z","iopub.status.idle":"2024-03-27T19:54:52.638100Z","shell.execute_reply.started":"2024-03-27T19:54:52.625791Z","shell.execute_reply":"2024-03-27T19:54:52.636863Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"age             int64\njob          category\nmarital      category\neducation    category\ndefault        object\nbalance         int64\nhousing        object\nloan           object\ncontact      category\nday             int64\nmonth        category\nduration        int64\ncampaign        int64\npdays           int64\nprevious        int64\npoutcome     category\ny            category\ndtype: object"},"metadata":{}}]},{"cell_type":"code","source":"train['loan'] = train['loan'].map( {'yes': 1, 'no': 0} ).astype(int)\ntrain['housing'] = train['housing'].map( {'yes': 1, 'no': 0} ).astype(int)\ntrain['default'] = train['default'].map( {'yes': 1, 'no': 0} ).astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.639717Z","iopub.execute_input":"2024-03-27T19:54:52.640773Z","iopub.status.idle":"2024-03-27T19:54:52.657789Z","shell.execute_reply.started":"2024-03-27T19:54:52.640741Z","shell.execute_reply":"2024-03-27T19:54:52.656564Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"test['loan'] = test['loan'].map( {'yes': 1, 'no': 0} ).astype(int)\ntest['housing'] = test['housing'].map( {'yes': 1, 'no': 0} ).astype(int)\ntest['default'] = test['default'].map( {'yes': 1, 'no': 0} ).astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.664013Z","iopub.execute_input":"2024-03-27T19:54:52.664381Z","iopub.status.idle":"2024-03-27T19:54:52.674962Z","shell.execute_reply.started":"2024-03-27T19:54:52.664353Z","shell.execute_reply":"2024-03-27T19:54:52.674094Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.676093Z","iopub.execute_input":"2024-03-27T19:54:52.676648Z","iopub.status.idle":"2024-03-27T19:54:52.696795Z","shell.execute_reply.started":"2024-03-27T19:54:52.676620Z","shell.execute_reply":"2024-03-27T19:54:52.695726Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"   age           job  marital  education  default  balance  housing  loan  \\\n0   58    management  married   tertiary        0     2143        1     0   \n1   44    technician   single  secondary        0       29        1     0   \n2   33  entrepreneur  married  secondary        0        2        1     1   \n3   47   blue-collar  married    unknown        0     1506        1     0   \n4   33       unknown   single    unknown        0        1        0     0   \n\n   contact  day month  duration  campaign  pdays  previous poutcome   y  \n0  unknown    5   may       261         1     -1         0  unknown  no  \n1  unknown    5   may       151         1     -1         0  unknown  no  \n2  unknown    5   may        76         1     -1         0  unknown  no  \n3  unknown    5   may        92         1     -1         0  unknown  no  \n4  unknown    5   may       198         1     -1         0  unknown  no  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>job</th>\n      <th>marital</th>\n      <th>education</th>\n      <th>default</th>\n      <th>balance</th>\n      <th>housing</th>\n      <th>loan</th>\n      <th>contact</th>\n      <th>day</th>\n      <th>month</th>\n      <th>duration</th>\n      <th>campaign</th>\n      <th>pdays</th>\n      <th>previous</th>\n      <th>poutcome</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>58</td>\n      <td>management</td>\n      <td>married</td>\n      <td>tertiary</td>\n      <td>0</td>\n      <td>2143</td>\n      <td>1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>5</td>\n      <td>may</td>\n      <td>261</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>44</td>\n      <td>technician</td>\n      <td>single</td>\n      <td>secondary</td>\n      <td>0</td>\n      <td>29</td>\n      <td>1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>5</td>\n      <td>may</td>\n      <td>151</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>33</td>\n      <td>entrepreneur</td>\n      <td>married</td>\n      <td>secondary</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>unknown</td>\n      <td>5</td>\n      <td>may</td>\n      <td>76</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>47</td>\n      <td>blue-collar</td>\n      <td>married</td>\n      <td>unknown</td>\n      <td>0</td>\n      <td>1506</td>\n      <td>1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>5</td>\n      <td>may</td>\n      <td>92</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33</td>\n      <td>unknown</td>\n      <td>single</td>\n      <td>unknown</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>5</td>\n      <td>may</td>\n      <td>198</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>no</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Define categorical columns for one-hot encoding\ncategorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n\n# Perform one-hot encoding\nencoder = OneHotEncoder(sparse=False, drop='first')  # 'drop' to avoid multicollinearity\nencoded_cols = pd.DataFrame(encoder.fit_transform(train[categorical_cols]))\n\n# Get the feature names after one-hot encoding\nencoded_feature_names = encoder.get_feature_names_out(categorical_cols)\nencoded_cols.columns = encoded_feature_names\n\n# Replace original categorical columns with encoded columns\ntrain_encoded = pd.concat([train.drop(columns=categorical_cols), encoded_cols], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.698302Z","iopub.execute_input":"2024-03-27T19:54:52.698647Z","iopub.status.idle":"2024-03-27T19:54:52.878769Z","shell.execute_reply.started":"2024-03-27T19:54:52.698618Z","shell.execute_reply":"2024-03-27T19:54:52.877723Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Define categorical columns for one-hot encoding\ncategorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n\n# Perform one-hot encoding\nencoder = OneHotEncoder(sparse=False, drop='first')  # 'drop' to avoid multicollinearity\nencoded_cols = pd.DataFrame(encoder.fit_transform(test[categorical_cols]))\n\n# Get the feature names after one-hot encoding\nencoded_feature_names = encoder.get_feature_names_out(categorical_cols)\nencoded_cols.columns = encoded_feature_names\n\n# Replace original categorical columns with encoded columns\ntest_encoded = pd.concat([test.drop(columns=categorical_cols), encoded_cols], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.880454Z","iopub.execute_input":"2024-03-27T19:54:52.881209Z","iopub.status.idle":"2024-03-27T19:54:52.911406Z","shell.execute_reply.started":"2024-03-27T19:54:52.881171Z","shell.execute_reply":"2024-03-27T19:54:52.910226Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"train_encoded.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.912602Z","iopub.execute_input":"2024-03-27T19:54:52.913185Z","iopub.status.idle":"2024-03-27T19:54:52.939142Z","shell.execute_reply.started":"2024-03-27T19:54:52.913156Z","shell.execute_reply":"2024-03-27T19:54:52.938095Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"   age  balance  day  duration  campaign  pdays  previous   y  \\\n0   58     2143    5       261         1     -1         0  no   \n1   44       29    5       151         1     -1         0  no   \n2   33        2    5        76         1     -1         0  no   \n3   47     1506    5        92         1     -1         0  no   \n4   33        1    5       198         1     -1         0  no   \n\n   job_blue-collar  job_entrepreneur  ...  month_jul  month_jun  month_mar  \\\n0              0.0               0.0  ...        0.0        0.0        0.0   \n1              0.0               0.0  ...        0.0        0.0        0.0   \n2              0.0               1.0  ...        0.0        0.0        0.0   \n3              1.0               0.0  ...        0.0        0.0        0.0   \n4              0.0               0.0  ...        0.0        0.0        0.0   \n\n   month_may  month_nov  month_oct  month_sep  poutcome_other  \\\n0        1.0        0.0        0.0        0.0             0.0   \n1        1.0        0.0        0.0        0.0             0.0   \n2        1.0        0.0        0.0        0.0             0.0   \n3        1.0        0.0        0.0        0.0             0.0   \n4        1.0        0.0        0.0        0.0             0.0   \n\n   poutcome_success  poutcome_unknown  \n0               0.0               1.0  \n1               0.0               1.0  \n2               0.0               1.0  \n3               0.0               1.0  \n4               0.0               1.0  \n\n[5 rows x 43 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>balance</th>\n      <th>day</th>\n      <th>duration</th>\n      <th>campaign</th>\n      <th>pdays</th>\n      <th>previous</th>\n      <th>y</th>\n      <th>job_blue-collar</th>\n      <th>job_entrepreneur</th>\n      <th>...</th>\n      <th>month_jul</th>\n      <th>month_jun</th>\n      <th>month_mar</th>\n      <th>month_may</th>\n      <th>month_nov</th>\n      <th>month_oct</th>\n      <th>month_sep</th>\n      <th>poutcome_other</th>\n      <th>poutcome_success</th>\n      <th>poutcome_unknown</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>58</td>\n      <td>2143</td>\n      <td>5</td>\n      <td>261</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>no</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>44</td>\n      <td>29</td>\n      <td>5</td>\n      <td>151</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>no</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>33</td>\n      <td>2</td>\n      <td>5</td>\n      <td>76</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>no</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>47</td>\n      <td>1506</td>\n      <td>5</td>\n      <td>92</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>no</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33</td>\n      <td>1</td>\n      <td>5</td>\n      <td>198</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>no</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows  43 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"scaler = StandardScaler()\nnumerical_cols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n\n# Fit and transform numerical columns using StandardScaler\ntrain_encoded[numerical_cols] = scaler.fit_transform(train_encoded[numerical_cols])","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.940573Z","iopub.execute_input":"2024-03-27T19:54:52.941114Z","iopub.status.idle":"2024-03-27T19:54:52.953897Z","shell.execute_reply.started":"2024-03-27T19:54:52.941085Z","shell.execute_reply":"2024-03-27T19:54:52.952898Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nnumerical_cols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n\n# Fit and transform numerical columns using StandardScaler\ntest_encoded[numerical_cols] = scaler.fit_transform(test_encoded[numerical_cols])","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.955110Z","iopub.execute_input":"2024-03-27T19:54:52.956248Z","iopub.status.idle":"2024-03-27T19:54:52.969391Z","shell.execute_reply.started":"2024-03-27T19:54:52.956109Z","shell.execute_reply":"2024-03-27T19:54:52.968321Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train_encoded['y'] = train_encoded['y'].map( {'yes': 1, 'no': 0} ).astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.970672Z","iopub.execute_input":"2024-03-27T19:54:52.971619Z","iopub.status.idle":"2024-03-27T19:54:52.979665Z","shell.execute_reply.started":"2024-03-27T19:54:52.971580Z","shell.execute_reply":"2024-03-27T19:54:52.978314Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"test_encoded['y'] = test_encoded['y'].map( {'yes': 1, 'no': 0} ).astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.981250Z","iopub.execute_input":"2024-03-27T19:54:52.981786Z","iopub.status.idle":"2024-03-27T19:54:52.992321Z","shell.execute_reply.started":"2024-03-27T19:54:52.981746Z","shell.execute_reply":"2024-03-27T19:54:52.991412Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"train_encoded.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:52.993538Z","iopub.execute_input":"2024-03-27T19:54:52.994509Z","iopub.status.idle":"2024-03-27T19:54:53.025129Z","shell.execute_reply.started":"2024-03-27T19:54:52.994475Z","shell.execute_reply":"2024-03-27T19:54:53.024040Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"        age   balance  day  duration  campaign     pdays  previous  y  \\\n0  1.606965  0.256419    5  0.011016 -0.569351 -0.411453  -0.25194  0   \n1  0.288529 -0.437895    5 -0.416127 -0.569351 -0.411453  -0.25194  0   \n2 -0.747384 -0.446762    5 -0.707361 -0.569351 -0.411453  -0.25194  0   \n3  0.571051  0.047205    5 -0.645231 -0.569351 -0.411453  -0.25194  0   \n4 -0.747384 -0.447091    5 -0.233620 -0.569351 -0.411453  -0.25194  0   \n\n   job_blue-collar  job_entrepreneur  ...  month_jul  month_jun  month_mar  \\\n0              0.0               0.0  ...        0.0        0.0        0.0   \n1              0.0               0.0  ...        0.0        0.0        0.0   \n2              0.0               1.0  ...        0.0        0.0        0.0   \n3              1.0               0.0  ...        0.0        0.0        0.0   \n4              0.0               0.0  ...        0.0        0.0        0.0   \n\n   month_may  month_nov  month_oct  month_sep  poutcome_other  \\\n0        1.0        0.0        0.0        0.0             0.0   \n1        1.0        0.0        0.0        0.0             0.0   \n2        1.0        0.0        0.0        0.0             0.0   \n3        1.0        0.0        0.0        0.0             0.0   \n4        1.0        0.0        0.0        0.0             0.0   \n\n   poutcome_success  poutcome_unknown  \n0               0.0               1.0  \n1               0.0               1.0  \n2               0.0               1.0  \n3               0.0               1.0  \n4               0.0               1.0  \n\n[5 rows x 43 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>balance</th>\n      <th>day</th>\n      <th>duration</th>\n      <th>campaign</th>\n      <th>pdays</th>\n      <th>previous</th>\n      <th>y</th>\n      <th>job_blue-collar</th>\n      <th>job_entrepreneur</th>\n      <th>...</th>\n      <th>month_jul</th>\n      <th>month_jun</th>\n      <th>month_mar</th>\n      <th>month_may</th>\n      <th>month_nov</th>\n      <th>month_oct</th>\n      <th>month_sep</th>\n      <th>poutcome_other</th>\n      <th>poutcome_success</th>\n      <th>poutcome_unknown</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.606965</td>\n      <td>0.256419</td>\n      <td>5</td>\n      <td>0.011016</td>\n      <td>-0.569351</td>\n      <td>-0.411453</td>\n      <td>-0.25194</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.288529</td>\n      <td>-0.437895</td>\n      <td>5</td>\n      <td>-0.416127</td>\n      <td>-0.569351</td>\n      <td>-0.411453</td>\n      <td>-0.25194</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.747384</td>\n      <td>-0.446762</td>\n      <td>5</td>\n      <td>-0.707361</td>\n      <td>-0.569351</td>\n      <td>-0.411453</td>\n      <td>-0.25194</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.571051</td>\n      <td>0.047205</td>\n      <td>5</td>\n      <td>-0.645231</td>\n      <td>-0.569351</td>\n      <td>-0.411453</td>\n      <td>-0.25194</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.747384</td>\n      <td>-0.447091</td>\n      <td>5</td>\n      <td>-0.233620</td>\n      <td>-0.569351</td>\n      <td>-0.411453</td>\n      <td>-0.25194</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows  43 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"code","source":"X_train = train_encoded.drop('y', axis=1)  # Features\ny_train = train_encoded['y']  # Target variable","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:53.026439Z","iopub.execute_input":"2024-03-27T19:54:53.026755Z","iopub.status.idle":"2024-03-27T19:54:53.036711Z","shell.execute_reply.started":"2024-03-27T19:54:53.026729Z","shell.execute_reply":"2024-03-27T19:54:53.035635Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"X_test = test_encoded.drop('y', axis=1)  # Features\ny_test = test_encoded['y']","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:53.037833Z","iopub.execute_input":"2024-03-27T19:54:53.041644Z","iopub.status.idle":"2024-03-27T19:54:53.048538Z","shell.execute_reply.started":"2024-03-27T19:54:53.041602Z","shell.execute_reply":"2024-03-27T19:54:53.047667Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Initialize the Random Forest Classifier\nrf_model = RandomForestClassifier(class_weight='balanced', random_state=42)\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [10, 25, 50, 75, 100],\n    'max_depth': [1,2, 5, 10, 15],\n    'min_samples_split': [0.5,1,2,5,10],\n    'min_samples_leaf': [0.1, 0.5, 1, 2],\n}\n\n# Initialize Grid Search Cross-Validation\ngrid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy')\n\n# Perform Grid Search CV\ngrid_search.fit(X_train, y_train)\n\n# Get the best hyperparameters\nbest_params = grid_search.best_params_\nprint(\"Best Hyperparameters:\", best_params)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:54:53.049990Z","iopub.execute_input":"2024-03-27T19:54:53.050768Z","iopub.status.idle":"2024-03-27T20:20:12.478925Z","shell.execute_reply.started":"2024-03-27T19:54:53.050739Z","shell.execute_reply":"2024-03-27T20:20:12.477506Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Best Hyperparameters: {'max_depth': 5, 'min_samples_leaf': 0.1, 'min_samples_split': 2, 'n_estimators': 25}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize Random Forest Classifier with best hyperparameters\nbest_rf_model = RandomForestClassifier(n_estimators=25, max_depth=5, min_samples_split=2, min_samples_leaf=0.1, random_state=42)\n\n# Train the model on the entire training dataset\nbest_rf_model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T21:45:06.717856Z","iopub.execute_input":"2024-03-27T21:45:06.718270Z","iopub.status.idle":"2024-03-27T21:45:07.023109Z","shell.execute_reply.started":"2024-03-27T21:45:06.718235Z","shell.execute_reply":"2024-03-27T21:45:07.021549Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize Random Forest Classifier with best hyperparameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m best_rf_model \u001b[38;5;241m=\u001b[39m \u001b[43mRandomForestClassifier\u001b[49m(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model on the entire training dataset\u001b[39;00m\n\u001b[1;32m      5\u001b[0m best_rf_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n","\u001b[0;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"],"ename":"NameError","evalue":"name 'RandomForestClassifier' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Make predictions on the test data\ny_pred_rf = best_rf_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T21:45:07.183818Z","iopub.execute_input":"2024-03-27T21:45:07.184257Z","iopub.status.idle":"2024-03-27T21:45:07.206324Z","shell.execute_reply.started":"2024-03-27T21:45:07.184224Z","shell.execute_reply":"2024-03-27T21:45:07.205199Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Make predictions on the test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m y_pred_rf \u001b[38;5;241m=\u001b[39m \u001b[43mbest_rf_model\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test)\n","\u001b[0;31mNameError\u001b[0m: name 'best_rf_model' is not defined"],"ename":"NameError","evalue":"name 'best_rf_model' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Calculate accuracy on test data\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\n\nprint(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")\n\n# Generate classification report on test data\nprint(\"\\nRandom Forest Classification Report:\")\nprint(classification_report(y_test, y_pred_rf))","metadata":{"execution":{"iopub.status.busy":"2024-03-27T21:45:07.673810Z","iopub.execute_input":"2024-03-27T21:45:07.674192Z","iopub.status.idle":"2024-03-27T21:45:07.705119Z","shell.execute_reply.started":"2024-03-27T21:45:07.674162Z","shell.execute_reply":"2024-03-27T21:45:07.703603Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate accuracy on test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m accuracy_rf \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m(y_test, y_pred_rf)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_rf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Generate classification report on test data\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'accuracy_score' is not defined"],"ename":"NameError","evalue":"name 'accuracy_score' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Calculate confusion matrix\ncm = confusion_matrix(y_test, y_pred_rf)\n\n# Create a heatmap for the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix - Random Forest')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T20:20:13.165853Z","iopub.execute_input":"2024-03-27T20:20:13.166607Z","iopub.status.idle":"2024-03-27T20:20:13.467591Z","shell.execute_reply.started":"2024-03-27T20:20:13.166571Z","shell.execute_reply":"2024-03-27T20:20:13.466697Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# Perform cross-validation with the best model\ncv_scores = cross_val_score(best_rf_model, X_train, y_train, cv=5)\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(f\"Mean CV Accuracy: {cv_scores.mean():.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-27T20:20:13.468914Z","iopub.execute_input":"2024-03-27T20:20:13.469482Z","iopub.status.idle":"2024-03-27T20:20:16.086362Z","shell.execute_reply.started":"2024-03-27T20:20:13.469450Z","shell.execute_reply":"2024-03-27T20:20:16.085163Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Cross-Validation Scores: [0.88300343 0.87889847 0.87060385 0.8892944  0.86485291]\nMean CV Accuracy: 0.8773\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get feature importances\nfeature_importances = best_rf_model.feature_importances_\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=feature_importances, y=X_train.columns)\nplt.xlabel('Feature Importance')\nplt.ylabel('Features')\nplt.title('Random Forest Feature Importance')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T20:20:16.088037Z","iopub.execute_input":"2024-03-27T20:20:16.088743Z","iopub.status.idle":"2024-03-27T20:20:16.728598Z","shell.execute_reply.started":"2024-03-27T20:20:16.088702Z","shell.execute_reply":"2024-03-27T20:20:16.727709Z"},"trusted":true},"execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"import xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2024-03-27T20:20:16.729865Z","iopub.execute_input":"2024-03-27T20:20:16.730378Z","iopub.status.idle":"2024-03-27T20:20:16.972848Z","shell.execute_reply.started":"2024-03-27T20:20:16.730330Z","shell.execute_reply":"2024-03-27T20:20:16.971682Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"param_grid = {\n    'max_depth': [3, 6, 9],\n    'learning_rate': [0.1, 0.01],\n    'n_estimators': [100, 200, 300],\n    'subsample': [0.8, 1.0],\n    'colsample_bytree': [0.8, 1.0],\n}","metadata":{"execution":{"iopub.status.busy":"2024-03-27T20:20:16.974553Z","iopub.execute_input":"2024-03-27T20:20:16.974984Z","iopub.status.idle":"2024-03-27T20:20:16.981362Z","shell.execute_reply.started":"2024-03-27T20:20:16.974948Z","shell.execute_reply":"2024-03-27T20:20:16.980082Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# Convert data into DMatrix format for XGBoost\ndtrain = xgb.DMatrix(X_train, label=y_train)\n\n# Initialize XGBoost model\nxgb_model = xgb.XGBClassifier(scale_pos_weight=8,objective='binary:logistic', seed=42)\n\n# Initialize Grid Search Cross-Validation\ngrid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='accuracy')\n\n# Perform Grid Search CV\ngrid_search.fit(X_train, y_train)\n\n# Print best parameters\nprint(\"Best Parameters:\", grid_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T20:20:16.982867Z","iopub.execute_input":"2024-03-27T20:20:16.983774Z","iopub.status.idle":"2024-03-27T20:28:45.670734Z","shell.execute_reply.started":"2024-03-27T20:20:16.983734Z","shell.execute_reply":"2024-03-27T20:28:45.669171Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Make predictions on the test set using the best model\nbest_xgb_model = grid_search.best_estimator_\ny_pred_xgb = best_xgb_model.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred_xgb)\nprint(f\"Accuracy: {accuracy:.4f}\")\n\n# Generate classification report and confusion matrix\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_xgb))","metadata":{"execution":{"iopub.status.busy":"2024-03-27T20:28:45.672254Z","iopub.execute_input":"2024-03-27T20:28:45.672716Z","iopub.status.idle":"2024-03-27T20:28:45.712228Z","shell.execute_reply.started":"2024-03-27T20:28:45.672679Z","shell.execute_reply":"2024-03-27T20:28:45.711100Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Accuracy: 0.7658\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.97      0.76      0.85      4000\n           1       0.31      0.85      0.45       521\n\n    accuracy                           0.77      4521\n   macro avg       0.64      0.80      0.65      4521\nweighted avg       0.90      0.77      0.81      4521\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Perform cross-validation with the best model\ncv_scores = cross_val_score(best_xgb_model, X_train, y_train, cv=5)\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(f\"Mean CV Accuracy: {cv_scores.mean():.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-27T20:28:45.713521Z","iopub.execute_input":"2024-03-27T20:28:45.713839Z","iopub.status.idle":"2024-03-27T20:28:48.252204Z","shell.execute_reply.started":"2024-03-27T20:28:45.713814Z","shell.execute_reply":"2024-03-27T20:28:48.251375Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Cross-Validation Scores: [0.82151941 0.71543906 0.66887857 0.49646096 0.5642557 ]\nMean CV Accuracy: 0.6533\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate confusion matrix\ncm_xgb = confusion_matrix(y_test, y_pred_xgb)\n\n# Create a heatmap for the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix - XGBoost')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T20:28:48.253704Z","iopub.execute_input":"2024-03-27T20:28:48.254357Z","iopub.status.idle":"2024-03-27T20:28:48.537145Z","shell.execute_reply.started":"2024-03-27T20:28:48.254307Z","shell.execute_reply":"2024-03-27T20:28:48.535780Z"},"trusted":true},"execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"import lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2024-03-27T20:28:48.538965Z","iopub.execute_input":"2024-03-27T20:28:48.539446Z","iopub.status.idle":"2024-03-27T20:28:49.760081Z","shell.execute_reply.started":"2024-03-27T20:28:48.539407Z","shell.execute_reply":"2024-03-27T20:28:49.757651Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# Define parameter grid for Grid Search\nparam_grid = {\n    'max_depth': [3, 6, 9],\n    'learning_rate': [0.1, 0.01],\n    'n_estimators': [100, 200, 300],\n    'subsample': [0.8, 1.0],\n    'colsample_bytree': [0.8, 1.0],\n}","metadata":{"execution":{"iopub.status.busy":"2024-03-27T20:28:49.761986Z","iopub.execute_input":"2024-03-27T20:28:49.762713Z","iopub.status.idle":"2024-03-27T20:28:49.769848Z","shell.execute_reply.started":"2024-03-27T20:28:49.762672Z","shell.execute_reply":"2024-03-27T20:28:49.768165Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# Initialize LightGBM model\nlgb_model = lgb.LGBMClassifier(is_unbalance=True,random_state=42)\n\n# Initialize Grid Search Cross-Validation\ngrid_search = GridSearchCV(estimator=lgb_model, param_grid=param_grid, cv=5, scoring='accuracy')\n\n# Perform Grid Search CV\ngrid_search.fit(X_train, y_train)\n\n# Print best parameters\nprint(\"Best Parameters:\", grid_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T20:28:49.771536Z","iopub.execute_input":"2024-03-27T20:28:49.772802Z","iopub.status.idle":"2024-03-27T20:37:17.178657Z","shell.execute_reply.started":"2024-03-27T20:28:49.772750Z","shell.execute_reply":"2024-03-27T20:37:17.177178Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007656 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010666 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010957 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010910 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010774 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010747 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010711 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010884 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010680 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010685 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010682 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010632 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010758 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010641 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010495 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011219 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010705 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011056 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013990 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010388 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011881 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010693 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010833 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010538 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010408 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010970 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010569 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010895 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010545 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010489 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010750 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010545 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010673 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010497 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010512 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007006 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010968 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010930 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010947 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010721 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010787 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011219 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010860 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006228 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013832 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010749 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010729 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010867 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010535 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010731 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010802 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010584 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010788 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010662 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010624 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010680 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010575 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010945 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011704 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010467 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010678 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011171 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008134 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010482 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010688 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011348 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010764 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010766 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010467 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010456 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010743 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010606 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012600 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010633 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010610 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011278 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010759 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012594 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006436 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010474 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010988 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010671 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010865 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010847 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010796 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014703 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010756 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010701 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010511 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010417 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010936 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010686 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010958 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010585 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010724 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010716 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010797 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011010 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010509 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010459 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010786 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010961 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010839 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010718 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006157 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011426 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010727 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010791 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010531 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010539 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010949 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010751 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010875 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010583 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010483 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010948 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010829 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010789 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010595 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010422 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010762 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010720 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011069 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010590 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010748 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011006 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010529 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010897 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010452 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010395 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011007 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011190 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011298 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010763 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010721 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010743 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010784 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011617 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015015 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010746 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010675 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010939 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006660 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010541 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010515 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010748 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010712 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010844 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010723 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013942 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010713 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010529 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010787 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010474 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010700 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010653 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010594 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010685 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010425 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010513 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010672 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010609 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010817 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010674 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010423 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010893 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010703 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010764 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010562 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011062 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010914 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010645 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011161 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010568 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010472 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010746 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011014 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010744 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010488 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010437 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007242 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007593 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006847 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006545 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006281 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008687 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007440 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006868 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006553 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006490 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007356 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007527 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006842 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006472 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006333 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007288 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007365 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006740 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006999 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006294 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007311 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007278 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006695 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006366 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006272 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007242 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007258 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006756 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006403 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006296 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007342 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007282 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006757 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006500 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006274 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008573 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007322 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006728 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006454 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006257 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007285 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007245 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006705 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006393 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006311 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007249 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007321 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006751 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006417 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006545 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007308 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007320 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006692 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006402 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006251 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007230 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007262 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006738 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006423 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006319 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007364 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007311 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006841 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006408 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006294 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007283 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007266 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006720 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006404 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006290 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007249 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007299 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006810 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006517 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006255 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008179 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007400 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006742 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006525 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006394 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007473 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007299 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006742 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007902 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006284 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007341 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007439 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006824 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006657 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006300 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007302 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007241 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006764 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006392 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006327 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007244 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007316 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006829 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006377 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006291 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007277 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007471 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006725 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006434 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006296 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007309 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007515 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006697 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006498 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006363 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007299 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009085 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006812 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006394 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006418 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007495 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007627 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006885 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006904 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006394 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007249 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007297 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006736 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006380 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006292 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007287 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007361 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006721 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006458 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006295 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007358 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007305 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006693 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006387 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006284 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007220 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009228 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006775 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006386 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006303 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007301 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007302 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006790 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006766 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006311 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007240 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007305 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006706 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006485 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006277 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007295 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007260 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006761 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008148 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006540 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007276 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007261 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006800 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006375 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006332 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007279 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007287 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006789 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006722 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006679 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007283 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007264 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007204 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006723 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006377 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007409 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007305 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006743 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006440 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006273 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007352 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007655 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006831 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006513 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006309 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 5289, number of negative: 39922\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013058 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1019\n[LightGBM] [Info] Number of data points in the train set: 45211, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116985 -> initscore=-2.021298\n[LightGBM] [Info] Start training from score -2.021298\nBest Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Make predictions on the test set using the best model\nbest_lgb_model = grid_search.best_estimator_\ny_pred_lgb = best_lgb_model.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred_lgb)\nprint(f\"Accuracy: {accuracy:.4f}\")\n\n# Generate classification report and confusion matrix\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_lgb))","metadata":{"execution":{"iopub.status.busy":"2024-03-27T20:37:17.180620Z","iopub.execute_input":"2024-03-27T20:37:17.181055Z","iopub.status.idle":"2024-03-27T20:37:17.224888Z","shell.execute_reply.started":"2024-03-27T20:37:17.181015Z","shell.execute_reply":"2024-03-27T20:37:17.223465Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Accuracy: 0.8938\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.95      0.94      4000\n           1       0.54      0.49      0.51       521\n\n    accuracy                           0.89      4521\n   macro avg       0.74      0.72      0.73      4521\nweighted avg       0.89      0.89      0.89      4521\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Perform cross-validation with the best model\ncv_scores = cross_val_score(best_lgb_model, X_train, y_train, cv=5)\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(f\"Mean CV Accuracy: {cv_scores.mean():.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-27T20:37:17.226368Z","iopub.execute_input":"2024-03-27T20:37:17.227401Z","iopub.status.idle":"2024-03-27T20:37:20.564650Z","shell.execute_reply.started":"2024-03-27T20:37:17.227364Z","shell.execute_reply":"2024-03-27T20:37:20.563421Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 4231, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011015 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36168, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116982 -> initscore=-2.021327\n[LightGBM] [Info] Start training from score -2.021327\n[LightGBM] [Info] Number of positive: 4232, number of negative: 31937\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010792 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1017\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117006 -> initscore=-2.021091\n[LightGBM] [Info] Start training from score -2.021091\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010679 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1013\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010924 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1016\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 4231, number of negative: 31938\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010671 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1012\n[LightGBM] [Info] Number of data points in the train set: 36169, number of used features: 42\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.116979 -> initscore=-2.021358\n[LightGBM] [Info] Start training from score -2.021358\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nCross-Validation Scores: [0.89726861 0.84406105 0.82990489 0.81209909 0.83167441]\nMean CV Accuracy: 0.8430\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate confusion matrix\ncm_lgb = confusion_matrix(y_test, y_pred_lgb)\n\n# Create a heatmap for the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm_lgb, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix - LGBoost')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T20:37:20.566166Z","iopub.execute_input":"2024-03-27T20:37:20.566627Z","iopub.status.idle":"2024-03-27T20:37:20.863453Z","shell.execute_reply.started":"2024-03-27T20:37:20.566584Z","shell.execute_reply":"2024-03-27T20:37:20.862091Z"},"trusted":true},"execution_count":58,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"cm_lgb","metadata":{"execution":{"iopub.status.busy":"2024-03-27T20:37:20.864890Z","iopub.execute_input":"2024-03-27T20:37:20.865227Z","iopub.status.idle":"2024-03-27T20:37:20.876316Z","shell.execute_reply.started":"2024-03-27T20:37:20.865200Z","shell.execute_reply":"2024-03-27T20:37:20.874916Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"array([[3787,  213],\n       [ 267,  254]])"},"metadata":{}}]},{"cell_type":"code","source":"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}